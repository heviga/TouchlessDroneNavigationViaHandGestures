\chapter{Practical Part}
In our task, we heavily worked with MediaPipe. We used it for data collection, face recognition, and finally gesture recognition. 

\section{Mediapipe}
MediaPipe is a framework used to create machine learning pipelines for time-series data like video and audio. Google initially developed it to process real-time video and audio analysis on YouTube \cite{mediapipe2019blog}. In 2019, the public release allowed researchers and developers to incorporate MediaPipe into their projects. Unlike other machine learning frameworks that require high computing power, MediaPipe can run efficiently on devices with low power, such as Android and IoT devices. It consists of the MediaPipe framework and MediaPipe solutions. The MediaPipe framework is developed using C++, Java, and Objective C programming. MediaPipe solutions include 16 pre-trained TensorFlow and TensorFlow Lite models built on top of the MediaPipe framework for specific use cases.
\subsection*{MediaPipe Hands}
MediaPipe Hands is a solution that tracks hands in real time. The problem of detecting hands is somewhat intricate. The model must be able to recognize hands and it must function across a wide range of hand sizes with a big scale span so it must be robust. It is rather difficult to identify hands based just on their visual traits since hands lack high-contrast patterns.% found on faces, such as those surrounding the lips and eyes.

MediaPipe Hands utilizes a combination of object detection, classification, and regression to recognize and track hands within a given image or video frame. The pipeline consists of two convolutional neural network models: a palm detector and a hand landmark model.


The palm detector is a single-shot detector model that uses an orientated hand-bounding box to locate palms on a whole input image. This is done before landmark detection because it is easier to estimate bounding boxes of rigid objects like palms than detecting hands with articulated fingers. It ignores many aspect ratios and so the bounding boxes are only squared. The model uses both classification (hands, no hands), and object detection (predicting a bounding box around the detected hand), a focal loss \cite{DBLP:journals/corr/abs-1708-02002} It was trained on data, containing 700 images of 14 geographical subregions, from both men and women.


The Hand Landmark Model runs subsequently with the palm detection model and uses regression to precisely localize 21 3D (x, y, z) coordinates within the hand region, similar to the data glove \ref{txt:glove}. Each landmark has a specific location on the hand that can vary within the pixel grid of an image. To capture the exact position, continuous values are used, allowing the model to indicate precisely where each landmark is located on the hand in terms of its x and y (and possibly z) coordinates within the image. The hands move and rotate freely, leading to a wide range of possible positions and orientations for each landmark. Continuous values are used because they make it possible to track hand gestures and movements more precisely. These gestures and movements may involve tiny changes to position and orientation that would be lost in discretized values.
The model is resilient to self-occlusions and partially visible hands, and it learns a consistent internal hand posture representation. The results from the model contain 21 hand landmarks consisting of x, y, and z, a hand flag that indicates the likelihood that a hand is present in the input image, and a binary system of handedness, e.g. left or right hand.
The coordinates x and y for each landmark are normalized \ref{sec:norm} to [0, 1] by image width and height. 

The model was trained both on real-world and synthetic datasets, noting that the wrist point was learned only from synthetic images. More than 30,000 manually annotated real-world images were used and the model is very robust, allowing it to detect and map hand landmark points accurately, even on partially visible hands in most cases. In Figure
\ref{fig:landmark_both_hands} my hands are not facing upfront but the model still finds all the landmarks for both hands. If tracking 2 hands, it also shows which one is left or right with different colors.

For encountering a tracking failure, There was also another model output created for tracking failures. It generates the likelihood that the given crop contains a hand that is reasonably aligned. The detector is triggered to reset tracking if the score falls below a predetermined threshold.

\begin{figure}
	\centering
	\includegraphics[width = 0.5\textwidth]{images/landmarks_both_hands.png}
	\caption{Hand landmarks model output}
	\label{fig:landmark_both_hands}
\end{figure}

\subsubsection*{Data Normalization}\label{sec:norm}
The MediaPipe hand landmarks model provides coordinates for hand landmark points based on the position of pixels containing those points in an image. As a result, the coordinates of two images of the same hand sign with different placements in the frame can have significantly different distances between them. This makes it more challenging to train the model.

To solve this problem, the wrist's landmark point has been considered with coordinates [0,0], and the coordinates of all other landmark points were adjusted accordingly.
First, the coordinates' values of the wrist's landmark point are subtracted from all coordinates' values.

Then, the coordinates were normalized to be between 0 and 1 by dividing them by the largest absolute value of the difference. Finally, the normalized coordinates were collected in the landmarks list. The coordinate normalization procedure is shown in Figure \ref{fig:normalization}.


\begin{figure}
	\centering
	\includegraphics[width = 0.85\textwidth]{images/normalise.pdf}
	\caption{Process of normalization of landmark coordinates}
	\label{fig:normalization}
\end{figure}

\subsection*{MediaPipe Face}
With support for multiple faces and six landmarks (left eye, right eye, nose tip, mouth, left eye region, and right eye region), MediaPipe Face Detection is a quick solution for face detection. Mediapipe BlazeFace \cite{bazarevsky2019blazeface} is a compact and efficient face detector designed for mobile GPU inference, that serves as its foundation, detecting only a few landmarks on the face and drawing a bounding box. Because of its real-time performance, the detector can be used with any live viewfinder experience that needs a precise facial region of interest as an input for other task-specific models, like face region segmentation, facial feature or expression classification, and 3D facial geometry estimation (like MediaPipe Face Mesh). BlazeFace leverages a resolution technique as an alternative to a GPU-friendly anchor mechanism adapted from Single Shot MultiBox Detector (SSD) \cite{Liu_2016} and a lightweight feature extraction network that is similar to MobileNetV1/V2 \cite{howard2017mobilenets}. With BlazeFace in use, we can get outputs shown in Figure \ref{fig:face_detections}, also outputting the confidence of the face.


\begin{figure}[ht]
	\centering
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{images/face_detection_good.png}
		%   \caption{Landmarks for pose estimation model by MediaPipe}
	\end{minipage}% <-- This percentage sign denotes the end of a minipage
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{images/face_detection.png}
		%   \caption{U-Net segmentation on an image from Oxford-IIIT Pet Dataset (Parkhi et al, 2012).}
	\end{minipage}
	\caption{Outputs of face detection model with different lightings}
	\label{fig:face_detections} % This label can now be used to reference both images together
\end{figure}


\section{Data Collection}
The flow of this project is as follows: the drone flies off, and we process the image that is captured by the drone's camera. Using multiple models for detecting, classifying, and localizing features in the image, after detecting a face, our model returns a prediction of a gesture and the drone executes the corresponding command in real time. The logic of the process is presented in Figure \ref{fig:flowchart}


\begin{figure}
	\centering
	\includegraphics[width = 0.9\textwidth]{images/flowchart.pdf}
	\caption{Logic in the recognition process.}
	\label{fig:flowchart}
\end{figure}


To make predictions, we had to train our model with data. Since we decided to use hand gestures, we had to collect the data ourselves. After finishing the training, we established drone commands and tested our program.

Although many publicly accessible datasets contain pictures of hand gestures, we chose to collect our data using the MediaPipe hand-tracking model. To do this, we recorded video footage of various hand gestures being held for a specific duration and moved around to increase diversity. Because the MediaPipe models are open-sourced and many developers use them for various projects, there are also numerous programs available for capturing and labeling gestures, like \cite{opencv_mediapipe_hand_gesture_recognition}. During the recording process, we labeled each gesture by pressing a key. Our dataset includes over 5000 samples, including images of both right and left hands, palms facing forward or backward toward the camera, and various degrees of hand positions captured by the camera.

\begin{figure}
	\centering
	\includegraphics[width = \textwidth]{images/all_gestures.pdf}
	\caption{Gestures used in this project.}
	\label{fig:all_gestures}
\end{figure}


Our final dataset has 8 total classes. It demonstrates the efficacy of the model and minimizes the risk of error commands that could lead to unsafe situations. The gestures are shown in Figure \ref{fig:all_gestures}.


The hand-tracking model used for data collection, outputs x, y, and z coordinates of hand landmark points from images, but only x and y coordinates are necessary for training the final model. Therefore, we eliminated the z coordinates. After normalization \ref{sec:norm}, we stored the remaining x and y coordinates of hand landmarks for various hand signs in a csv file for each hand landmark. The coordinates displayed in Figure \ref{fig:saved_csv} represent the data points used to generate the final dataset and train the final model. The first column denotes the gesture label, the next two columns indicate the coordinates of landmark 0 at the wrist, and so on. Our csv file contains many combinations of landmark coordinates for each gesture.


\begin{figure}[h]
	\centering
	\includegraphics[width = \textwidth]{images/gesture_laandmarks-cropped.pdf}
	\caption{A selection of the saved csv file}
	\label{fig:saved_csv}
\end{figure}


\section{Model}
\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[
		scale = 0.7,
		every node/.style={scale=0.6},
		plain/.style={
			draw=none,
			fill=none,
		},
		cir/.style={
			draw,
			circle,
			inner sep=8.5pt
		},
		dot/.style={
			draw,
			shape=circle,
			minimum size=3pt,
			inner sep=0,
			fill=black
		},
		net/.style={
			matrix of nodes,
			nodes={
				draw,
				circle,
				inner sep=8.5pt
			},
			nodes in empty cells,
			column sep=1cm,
			row sep=-5pt
		},
		arrow/.style={-Latex}
		]
		\matrix[net] (mat)%10x5
		{
			|[plain]| \parbox{1.5cm}{\centering Input\\layer}
			& |[plain]| \parbox{1.5cm}{\centering Hidden\\layer}
			& |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2}
			& |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 3}
			& |[plain]| \parbox{1.5cm}{\centering Output\\layer} \\
			|[cir]| &           |[plain]| &     |[plain]| &         |[plain]| &             |[cir]| \\
			|[plain]| &         |[plain]| &     |[plain]| &         |[plain]| &               |[plain]| \\
			|[cir]| &           |[cir]| &       |[cir]| &         |[cir]| &             |[cir]| \\
			|[plain]| &         |[plain]| &       |[plain]| &           |[plain]| &               |[plain]| \\
			|[plain]| &         |[dot]| &       |[dot]| &           |[dot]| &               |[cir]| \\
			|[dot]| &           |[plain]|   &     |[plain]| &           |[plain]| &               |[plain]| \\
			|[plain]| &         |[dot]| &     |[dot]| &         |[dot]| &             |[cir]| \\
			|[dot]| &           |[plain]| &       |[plain]| &           |[plain]| &               |[plain]| \\
			|[plain]| &         |[dot]| &     |[dot]| &         |[dot]| &             |[cir]| \\
			|[dot]| &           |[plain]| &     |[plain]| &         |[plain]| &             |[plain]| \\
			|[plain]| &         |[cir]| &       |[cir]| &           |[cir]| &               |[cir]| \\
			|[plain]| &         |[plain]| &     |[plain]| &         |[plain]| &             |[plain]| \\
			|[cir]| &           |[plain]| &     |[plain]| &         |[plain]| &             |[cir]| \\
			|[plain]| &         |[plain]| &     |[plain]| &         |[plain]| &             |[plain]| \\
			|[plain]| &         |[plain]| &     |[plain]| &         |[plain]| &             |[cir]| \\
			|[plain]| &         |[plain]| &     |[plain]| &         |[plain]| &             |[plain]| \\
		};
		
		% Connect the 'cir' nodes from Input Layer to Hidden Layer 1
		\draw[arrow] (mat-2-1) -- (mat-4-2);
		\draw[arrow] (mat-2-1) -- (mat-12-2);
		\draw[arrow] (mat-4-1) -- (mat-12-2);
		\draw[arrow] (mat-4-1) -- (mat-4-2);
		\draw[arrow] (mat-14-1) -- (mat-4-2);
		\draw[arrow] (mat-14-1) -- (mat-12-2);
		
		% Connect the 'cir' nodes from Hidden Layer 1 to Hidden Layer 2
		\draw[arrow] (mat-4-2) -- (mat-4-3);
		\draw[arrow] (mat-4-2) -- (mat-12-3);
		\draw[arrow] (mat-12-2) -- (mat-12-3);
		\draw[arrow] (mat-12-2) -- (mat-4-3);
		
		
		% Connect the 'cir' nodes from Hidden Layer 2 to Hidden Layer 3
		\draw[arrow] (mat-4-3) -- (mat-4-4);
		\draw[arrow] (mat-4-3) -- (mat-12-4);
		\draw[arrow] (mat-12-3) -- (mat-12-4);
		\draw[arrow] (mat-12-3) -- (mat-4-4);
		
		% Connect the 'cir' nodes from Hidden Layer 3 to Output Layer
		\draw[arrow] (mat-4-4) -- (mat-2-5);
		\draw[arrow] (mat-4-4) -- (mat-4-5);
		\draw[arrow] (mat-4-4) -- (mat-6-5);
		\draw[arrow] (mat-4-4) -- (mat-8-5);
		\draw[arrow] (mat-4-4) -- (mat-10-5);
		\draw[arrow] (mat-4-4) -- (mat-12-5);
		\draw[arrow] (mat-4-4) -- (mat-14-5);
		\draw[arrow] (mat-4-4) -- (mat-16-5);
		
		\draw[arrow] (mat-12-4) -- (mat-2-5);
		\draw[arrow] (mat-12-4) -- (mat-4-5);
		\draw[arrow] (mat-12-4) -- (mat-6-5);
		\draw[arrow] (mat-12-4) -- (mat-8-5);
		\draw[arrow] (mat-12-4) -- (mat-10-5);
		\draw[arrow] (mat-12-4) -- (mat-12-5);
		\draw[arrow] (mat-12-4) -- (mat-14-5);
		\draw[arrow] (mat-12-4) -- (mat-16-5);
		
		
		% Labels for cir nodes
		% Incoming arrows with labels for the input layer 'cir' nodes
		\draw[arrow] ([xshift=-1cm]mat-2-1.west) -- (mat-2-1) node[above, midway] {I1 };
		\draw[arrow] ([xshift=-1cm]mat-4-1.west) -- (mat-4-1) node[above, midway] {I2 };
		\draw[arrow] ([xshift=-1cm]mat-14-1.west) -- (mat-14-1) node[above, midway] {I42 };
		
		\node[above=0.5cm of mat-5-2] (hidden11) {H1};
		\node[above=0.7cm of mat-17-2] (hidden12) {H256};
		
		\node[above= 0.5cm of mat-5-3] (hidden21) {H1};
		\node[above= 0.7cm of mat-17-3] (hidden22) {H128};
		
		\node[above=0.5cm of mat-5-4] (hidden31) {H1};
		\node[above=0.7cm of mat-17-4] (hidden32) {H64};
		
		\draw[arrow] (mat-2-5.east) -- ([xshift=1cm]mat-2-5.east) node[above, midway] {0};
		\draw[arrow] (mat-4-5.east) -- ([xshift=1cm]mat-4-5.east) node[above, midway] {1};
		\draw[arrow] (mat-6-5.east) -- ([xshift=1cm]mat-6-5.east) node[above, midway] {2};
		\draw[arrow] (mat-8-5.east) -- ([xshift=1cm]mat-8-5.east) node[above, midway] {3};
		\draw[arrow] (mat-10-5.east) -- ([xshift=1cm]mat-10-5.east) node[above, midway] {4};
		\draw[arrow] (mat-12-5.east) -- ([xshift=1cm]mat-12-5.east) node[above, midway] {5};
		\draw[arrow] (mat-14-5.east) -- ([xshift=1cm]mat-14-5.east) node[above, midway] {6};
		\draw[arrow] (mat-16-5.east) -- ([xshift=1cm]mat-16-5.east) node[above, midway] {7};
		
		% ... (rest of your output labels)
		
		
	\end{tikzpicture}
	\caption{Neural network architecture.}
	\label{fig:neural_network}
\end{figure}

The model we used for gesture recognition is a fully connected feedforward neural network. The model is adjusted to work with data output by KeyPointClassifier. KeyPointClassifier is a class that processes data of MediaPipe Hands models. The data are then sent to our gesture classifier.


%\begin{figure}
%	\centering
%	\includegraphics[width = 0.65\textwidth]{images/model_summ.png}
%	\caption{Configuration of the used feedforward neural network}
%	\label{fig:layers}
%\end{figure}
The following is a report on training our neural network with the data. We utilized a model, which comprises one input layer and one output layer, along with five hidden layers consisting of three dense layers and two dropout layers. The sequential application of dropout and dense layers with ReLU activation acts as a mechanism to prevent overfitting while allowing the model to make nonlinear mappings. The output layer of the neural network comprises neurons equivalent to the number of hand gestures it can recognize - 8. The configuration of the model is depicted in Figure \ref{fig:neural_network}. The was trained with a dataset split into training and testing sets and was designed using the Adam optimizer, which is efficient and appropriate for training models with large data and parameters. The Sparse Categorical Crossentropy loss function was utilized to evaluate the loss between predicted and actual labels, while accuracy was used as the evaluation metric to determine how frequently the prediction matches the actual label. The training was halted by early stopping when the validation loss stopped to decrease and then converted into TensorFlow Lite to reduce its size. Initially, the training model was set to have only four hidden layers (2 dense layers). However, the training accuracy failed to exceed 90\%. Consequently, we updated the model to have five hidden layers (with 3 dense layers), without any other modifications and obtained satisfactory results. The training accuracy achieved was 99.62\%, and the calculated loss was 0.0309.





\section{Results}

To perform a quantitative analysis of the test dataset, we employed the  \text{classification report} and  \text{confusion matrix} libraries from scikit-learn. The \text{classification report} library produced an assessment report of our model with accuracy, precision, recall, and F1 score matrices. Additionally, the support matrix represents the model's real-time recognition performance.


\begin{figure}
	\centering
	\includegraphics[width = 0.65\textwidth]{images/classific_report.png}
	\caption{Classification report of the model}
	\label{fig:report}
\end{figure}


Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. 

The accuracy matrix calculates the number of correctly predicted labels by the model from the entire dataset (Eq.\ref{eq:accuracy}). 
The precision matrix measures the model's accuracy out of the predicted positives. It calculates the number of actual positives in the predicted positives and is an excellent measure to consider when the False Positive (FP) cost is high. Equation \ref{eq:precision} depicts the mathematical formula of the precision matrix.

The recall matrix measures the number of predictions our model correctly labels as positives. It is a measure considered when false negatives have high costs. The mathematical formulation of the recall matrix is Equation \ref{eq:recall}. 

The F1 score is calculated by combining both precision and recall, as shown in Equation \ref{eq:f1_score}. It is their harmonic mean.

Support is the number of samples in each class. 
The macro-average of precision, recall and F1-score demonstrates the system's average performance across all classes, while the weighted average takes into account the class imbalance by weighting the metrics based on the number of samples in each class.\newline
 Additionally, the loss value of $3.09\%$ (not shown) indicates how efficiently the model is minimizing its errors during training, while overall accuracy stands at $99.62\%$

The classification report of the implemented model in detail is shown in Figure \ref{fig:report}. 


\begin{equation}
	\text{accuracy} = \frac{TP + TN}{TP + TN + FP +FN}
	\label{eq:accuracy}
\end{equation}

\begin{equation}
	\text{precision} = \frac{TP}{TP + FP}
	\label{eq:precision}
\end{equation}
\begin{equation}
	\text{recall} = \frac{TP}{TP + FN}
	\label{eq:recall}
\end{equation}

\begin{equation}
	\text{F1 score} = \frac{2 P  R}{P+ R} 
	\label{eq:f1_score}
\end{equation}

In the equations (\ref{eq:accuracy}), (\ref{eq:precision}), (\ref{eq:recall}), and (\ref{eq:f1_score}), TP, TN, FP, and FN represent True Positive, True Negative, False Positive,
and False Negative, respectively.

\begin{figure}[h!]
	\centering
	\includegraphics[width = 0.65\textwidth]{images/confusion_matrix.pdf}
	\caption{Confusion matrix of the model}
	\label{fig:confusion_matrix}
\end{figure}


The confusion matrix is a key performance metric used in machine learning, particularly in classification tasks. By utilizing the scikit-learn library in Python, one can create a confusion matrix. To conduct our experiment on predicting hand gestures, we obtained and pre-processed the necessary datasets. The confusion matrix was then utilized to evaluate the accuracy of our model, which is illustrated in Figure \ref{fig:confusion_matrix}. The x-axis represents predicted labels, while the y-axis represents actual labels.

Through the confusion matrix, we can pinpoint which gestures are being incorrectly recognized. For instance, we observed that gesture 2 was misinterpreted as gesture 6 a total of 11 times. We can further calculate the recall of gesture 2 utilizing Equation \ref{eq:recall2}:

\begin{equation}
	\text{recall(2)} = \frac{545}{545 + 11}
	\label{eq:recall2}
\end{equation}



The system's performance is satisfactory, with almost perfect precision, recall, and F1-score in most classes, as well as high accuracy and weighted average.

To find the model's performance from a loss validation standpoint, we can plot its progress throughout the epochs.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{images/model_loss_relu.png}
	\caption{Training and validation loss curves for a neural network model utilizing the ReLU activation function.}
	\label{fig:model_loss_relu}
\end{figure}

The graph indicates a rapid decrease in loss in the initial epochs followed by a stable, low variance loss, demonstrating good model convergence without overfitting.

\subsection*{Comparisons}
As we wanted to optimize our neural network, we conducted a comparison of the Stochastic Gradient Descent (SGD) and Adam optimizers. Figure \ref{fig:optimizers} shows that Adam achieves faster convergence, reduces loss swiftly, and maintains high accuracy, making it suitable for complex neural network training where time and resource optimization are as critical as model accuracy. On the other hand, SGD shows more gradual improvement and requires more epochs to reach comparable performance.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{images/optimizers_comparison.png}
	\caption{Comparative analysis of different optimizers.}
	\label{fig:optimizers}
\end{figure}


We studied the impact of initial learning rates on Adam optimizer's performance. The initial learning rate has a significant effect on the convergence speed and training stability, as demonstrated in figure \ref{fig:learning_rates}.\newline
The model trained with a higher initial learning rate of $0.001$ (as used in our case) converges rapidly, as indicated by a swift decline in loss and a quick rise in accuracy. However, the models with lower initial rates of $0.0001$ and $1e^{-5}$ demonstrate a more gradual improvement, with the lowest rate showcasing the most stable but slowest progression.\newline
The initial learning rate serves as a multiplier to the adaptively computed rates and determines the starting point for adjustments.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{images/adam_learning_rates_comparison.png}
	\caption{Training and validation accuracy over epochs for different initial learning rates using the Adam optimizer.}
	\label{fig:learning_rates}
\end{figure}

We researched different batch sizes, ranging from 16 to 1024, to find the right one for our model. We found that smaller batch sizes, such as 16 and 64, decrease the loss quickly, but they are less stable and can make the convergence process more complicated. On the other hand, larger batch sizes, such as 512 and 1024, are more stable but converge slower, which means they may not use the training data as efficiently. The model's performance on each batch size is shown in figure \ref{fig:batch_sizes}

After considering all the options, we chose a batch size of 128. This size balances fast convergence with stable loss decrease and efficiently learns from the data. It is a good compromise between the noise observed in smaller batch sizes and the slow convergence observed in larger ones. The loss curve for a batch size of 128 is consistent, which means that our model has strong generalization performance. Therefore, we selected this batch size for our final model training.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{images/batch_sizes_comparison.png}
	\caption{The effect of batch size on model loss during training and validation.}
	\label{fig:batch_sizes}
\end{figure}

Figure \ref{fig:epochs} shows the model's loss over 1000 epochs when using the ReLU activation function. The training loss decreases rapidly in the initial epochs, indicating the model is learning quickly and stabilizing. However, the validation loss fluctuates significantly throughout the training process, particularly in later epochs. This suggests that the model may be capturing noise or overfitting to the training data, continuing to learn beyond achieving minimal loss on the validation set.

The importance of implementing early stopping as a regularization technique (like in figure \ref{fig:model_loss_relu}) is underscored by this extended training. Without it, the model risks wasting computational resources and reducing its ability to generalize. The increasing divergence between training and validation loss after several hundred epochs reinforces the need for early stopping to halt training when the model ceases to improve in generalization to validation data.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{images/model_loss_over_1000_epochs.png}
	\caption{Model loss using the ReLU activation function plotted over 1000 epochs without early stopping.}
	\label{fig:epochs}
\end{figure}



\section{Drone Implementation}

The Tello drone is a compact quadcopter that boasts an advanced vision positioning system and a high-resolution (8 mp) onboard camera, designed to capture stunning aerial photographs and videos. It offers versatile operating options, allowing users to control it through a laptop computer or a smartphone. The drone connects to devices using Wi-Fi, which facilitates a stable and fast connection essential for real-time video streaming and responsive flight control. Additionally, it supports the 2.4 GHz frequency band for remote control, a widely used standard that ensures reliable communication over distances. The drone has other built-in functions like range finder, barometer, LED or vision system. It is fitted with a single accelerometer and 3- axis magnetometer (which can get the drone orientation), a pressure
and IR-based altitude detector. The Tello drone is compatible with a dedicated smartphone application available on both Android and iOS platforms. This app unlocks a number of intuitive flight modes, easy-to-use controls, and creative functions.

The Tello library is a toolkit crafted to seamlessly integrate with Python. It's specifically designed for developers and hobbyists, offering easy access and control over the Tello drone's functionalities. With a comprehensive array of built-in functions, the library manages communication protocols with the drone, allowing for real-time transmission of control commands and reception of status updates. This ensures a smooth and responsive piloting experience.

Moreover, the Tello library efficiently handles state changes, adapting to various flight conditions and drone responses, such as battery levels, speed, and altitude. For example, if the drone is low on battery, it doesn't flip. This feature is essential for creating applications that require precise control and feedback from the drone, providing a foundation for developing flight patterns and maneuvers.

The library's event-driven control allows the execution of code in response to specific events, such as takeoff, landing, or moving in a direction with a designated speed.

\begin{figure}[h!]
	\centering
	\includegraphics[width = 0.6\textwidth]{images/drone.jpg}
	\caption{Drone used in this project.}
	\label{fig:tello}
\end{figure}

To operate a drone, we can use specific hand gestures as commands. This technique is chosen due to its ease of use and intuitive nature. Additionally, Tello provides a Python API that streamlines the process of drone operation. This API eliminates the need for direct manipulation of the motor hardware, allowing us to focus on perfecting our hand gestures and achieving more precise control of the drone. For this to work, we just set each gesture ID to a command.



Table \ref{tab:gesture_commands} lists the commands used in our project, which were assigned to the labels of our gestures. The gestures we utilized can be found in Figure \ref{fig:all_gestures}.

\begin{table}[ht]
	\centering
	\begin{tabular}{ll}
		\toprule
		Gesture Label & Command \\
		\midrule
		0 - Palm& Move along the y-axis (forward velocity) \\
		1 - Fist& Move along -y-axis (backward velocity) \\
		2 - Rock& Flip forward (upward velocity) \\
		3 - OK & Take-off or land (depending on whether in flight or landed) \\
		4 - Peace& Takes a picture (saves in png) \\
		5 - Like& Rotate 360$^\circ$ \\
		6 - Up& Move up (ascending velocity) \\
		7 - Down & Move down (descending velocity)\\
		\bottomrule
	\end{tabular}
	\caption{Gesture commands for drone control}
	\label{tab:gesture_commands}
\end{table}

Our program prompts the Tello drone to begin executing commands after a key press and identifies a gesture every 5 seconds. This allows for the comfortable execution of a command before recognizing the next one. After tweaking speeds and sleep times, the Tello drone executed the received commands rapidly and consistently.

In addition, if we are not recognizing any gesture, the drone is ready to land on the palm. Because the drone can execute most of the commands only while flying, it takes off at the start of the program. It can also take off with gesture 4, so the program doesn't have to be reset.


\section{Future Work}
Moving forward with our work, there's lot of space for the project's expansion. Since we already have a good classifier for gestures, we can use our model not only for the drone, but also for other applications like smart home control, sign language translation, multimedia control or security systems.\newline One simple study \cite{doi:10.1080/08839514.2023.2176607} used their gesture recognizer and ESP8266 microcontroller to send commands to home appliances, using gestures to control a simple LED light (turning  it on, off, setting a brightness and color).

If we went in the direction of implementing face recognition, we'd find many popular face recognition systems. Facial recognition systems are common in many aspects of modern life, making authentification processes easier, without the need of passwords or physical IDs.
Examples of algorithms used for face recognition include Eigenfaces \cite{CARIKCI2012118}, Local Binary Patterns Histograms (LBPH) \cite{inproceedings}, Fisherfaces, Scale Invariant Feature Transform (SIFT) \cite{articlefr}, and Speed Up Robust Features (SURF) \cite{10.1007/11744023_32}. Commonly, face recognition projects use LBPH algorithm integrated within the OpenCV library. For face detection alone, the Haar Cascade classifier by Paul Viola and Michael Jones \cite{inproceedings} is the first choice.

Since we work with Mediapipe it was the first choice to try use their Face Landmarker for extracting 478 3-dimensional face landmarks. This model works along with the BlazeFace detection model, as well as Face mesh model. The difference of ooutputs of each model is shown in Figure \ref{fig:face_landmark}.
We applied these model in similar ways as we did with the hand models, using spatial positions of the landmarks. Firstly, we collected our data, labeling each capture of face with a label. Then, we trained the model, which was a simple KNN model to predict the person in the picture. Since we don't have a large database (only tried with 2 people, since the prediction is real-time, the data consisted only of close people), it's hard to conclude whether the model would be sufficient for higher usage. However, it shows an alternate way for the face recognition, using only Mediapipe models. The face recognition model is saved in our repository \cite{touchlessdronecontrol}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/face_landmark.png}
	\caption{Outputs of FaceBlaze and Face Landmarker model.}
	\label{fig:face_landmark}
\end{figure}




